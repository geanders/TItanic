{
    "contents" : "---\ntitle: \"K-Nearest Neighbors\"\nauthor: \"Brooke Anderson\"\ndate: \"January 20, 2016\"\noutput: html_document\n---\n\n```{r echo = FALSE, message = FALSE}\nlibrary(knitr)\nopts_knit$set(fig.path = '../figures/knn-')\n```\n\nLoad required libraries.\n\n```{r message = FALSE}\nlibrary(dplyr) ## Data wrangling\nlibrary(class) ## Includes `knn` function\nlibrary(ggplot2)\n```\n\nRead in the data.\n\n```{r}\ntrain <- read.csv(\"../data/train.csv\") %>%\n  mutate(Survived = factor(Survived),\n         Pclass = factor(Pclass),\n         Name = as.character(Name),\n         Sex = factor(Sex))\ntest <- read.csv(\"../data/test.csv\") %>%\n  mutate(Pclass = factor(Pclass),\n         Name = as.character(Name),\n         Sex = factor(Sex))\n```\n\n## Model with k = 1 and single continuous predictor\n\nI started by fitting a model with a k of one, so just looking at the single nearest neighbor, and using only `Age` as a predictor. Here's the relationship between `Age` and `Survived` in the training data:\n\n```{r fig.width = 3, fig.height = 3.5}\nggplot(train, aes(x = Age)) + \n  geom_histogram(binwidth = 5) + \n  facet_wrap(~ Survived, ncol = 1) + \n  theme_minimal()\n```\n\nNext I fit the model. The model seemed to have some problems when the predictive variable had missing values, so I removed those from both testing and training data sets before fitting. \n\n```{r}\ntrain_knn <- filter(train, !is.na(Age))\ntrain_x <- select(train_knn, Age)\ntrain_y <- train_knn$Survived\ntest_x <- filter(test, !is.na(Age)) %>%\n  select(Age)\n```\n\nNext, I  fit the model, with k = 1. I fit it first for the training data, to get an estimate of that accuracy. (That means I repeat `train_x` in the model statement.)\n\n```{r}\nset.seed(1201)\ntrain_pred <- knn(train_x, train_x, train_y, k = 1)\nhead(train_pred)\n```\n\nThe `knn` function gives the predictions directly, unlike the function for Naive Bayes, which works more like a typical modeling structure for R. \n\nNow, to assess accuracy, I'll need to get these back into the original dataframe and pick which values I want to use for predictions when `Age` is missing. I'll use \"0\", since the majority of people in the training dataset did not survive. \n\nThis part is a bit tricky, because you have to merge back in with the missing values and align things correctly with the passenger IDs. Normally, I would have kept `PassengerId` in to do this, but `knn` was finicky about letting me do that without modeling it. \n\n```{r}\nx_accuracy <- data.frame(Survived = train$Survived,\n                         pred = factor(\"0\", levels = c(\"0\", \"1\")))\nx_accuracy$pred[!is.na(train$Age)] <- train_pred\nhead(cbind(x_accuracy, train$Age))\nmean(x_accuracy$Survived == x_accuracy$pred)\n```\n\nAccuracy for this model in the training set is `r round(mean(x_accuracy$Survived == x_accuracy$pred), 5)`.\n\nNext, I fit the same model to the testing data and check out the accuracy against the Kaggle Leaderboard. \n\n```{r}\nset.seed(1201)\ntest_pred <- knn(train_x, test_x, train_y, k = 1)\ntest_accuracy <- data.frame(PassengerId = test$PassengerId,\n                         Survived = factor(\"0\", levels = c(\"0\", \"1\")))\ntest_accuracy$Survived[!is.na(test$Age)] <- \n  as.numeric(as.character(test_pred))\nwrite.csv(test_accuracy, file = \"../predictions/knn_age_k1.csv\",\n          row.names = FALSE)\n```\n\nMy score on Kaggle was 0.57416. This was my worst model to date. ",
    "created" : 1453351272957.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3352400882",
    "id" : "C64C0E5D",
    "lastKnownWriteTime" : 1453357758,
    "path" : "~/Desktop/MachineLearningClass/Titanic/Rscripts/KNN.Rmd",
    "project_path" : "Rscripts/KNN.Rmd",
    "properties" : {
        "tempName" : "Untitled2"
    },
    "relative_order" : 3,
    "source_on_save" : false,
    "type" : "r_markdown"
}